{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal hyperparameters for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn import metrics   # Additional sklearn functions\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import random\n",
    "from decimal import Decimal\n",
    "\n",
    "sys.path.append('..')\n",
    "from Workflow import Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = Workflow()\n",
    "Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = wf.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgbfit(alg, Xtrain, ytrain, cv_folds=5, early_stopping_rounds=50):\n",
    "    \"\"\"\n",
    "    Fits the XGBoost model using cross-validation to determine the optimal number of boosting rounds.\n",
    "    \n",
    "    Parameters:\n",
    "    alg: The XGBoost model to be trained (should be an instance of XGBRegressor).\n",
    "    Xtrain: Training features (input dataset).\n",
    "    ytrain: Training target variable (output labels).\n",
    "    cv_folds: Number of folds for cross-validation (default is 5).\n",
    "    early_stopping_rounds: Number of rounds without improvement to stop training (default is 50).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the parameters of the XGBoost model for cross-validation\n",
    "    xgb_param = alg.get_xgb_params()\n",
    "        \n",
    "    # Convert the training data to the DMatrix format required by XGBoost\n",
    "    xgtrain = xgb.DMatrix(Xtrain, ytrain)\n",
    "        \n",
    "    # Perform cross-validation to find the optimal number of boosting rounds\n",
    "    cvresult = xgb.cv(\n",
    "        xgb_param, \n",
    "        xgtrain, \n",
    "        num_boost_round=alg.get_params()['n_estimators'],  # Evaluate this many boosting rounds\n",
    "        nfold=cv_folds,  # Number of folds for cross-validation\n",
    "        metrics='rmse',  # Metric to optimize (Root Mean Squared Error)\n",
    "        early_stopping_rounds=early_stopping_rounds  # Stop training if no improvement over this number of rounds\n",
    "    )\n",
    "        \n",
    "    # Update the model's n_estimators to the optimal number found from cross-validation\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    print(f'Optimal n_estimators: {cvresult.shape[0]}')\n",
    "\n",
    "    # Fit the model on the entire training dataset with the optimized number of boosting rounds\n",
    "    alg.fit(Xtrain, ytrain)\n",
    "        \n",
    "    # Make predictions on the training set using the trained model\n",
    "    dtrain_predictions = alg.predict(Xtrain)\n",
    "        \n",
    "    # Print a report summarizing model performance\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error(ytrain, dtrain_predictions)))  # Calculate RMSE\n",
    "    print(\"R2 Score : %.4g\" % metrics.r2_score(ytrain, dtrain_predictions))  # Calculate R2 Score\n",
    "                    \n",
    "    # Calculate feature importances using the trained model's booster\n",
    "    # 'importance_type' defines how feature importances are calculated (here using weight)\n",
    "    feat_imp = pd.Series(alg.get_booster().get_score(importance_type='weight')).sort_values(ascending=False)\n",
    "    \n",
    "    # Plot the feature importances\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')  # Set the y-axis label for the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "xgb1 = XGBRegressor(\n",
    " learning_rate=0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective='reg:squarederror',  # Changed to regression objective\n",
    " nthread=4,\n",
    " seed=42)\n",
    "\n",
    "train_data_file = 'train_data.csv'\n",
    "train_data = pd.read_csv(train_data_file)\n",
    "train_params = ['Rain mm/y', 'rainfall_seasonality', 'PET mm/y', 'elevation_mahd', 'distance_to_coast_km', 'ndvi_avg', 'clay_perc', 'soil_class']\n",
    "y_var='Recharge RC 50% mm/y'\n",
    "Xtrain = train_data[train_params]\n",
    "ytrain = train_data[y_var]\n",
    "\n",
    "xgbfit(xgb1, Xtrain, ytrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsaie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
