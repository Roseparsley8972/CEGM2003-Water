{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb_model(t_size=0.3, n_estimators=350, max_depth=12, learning_rate=0.01, min_child_weight=5, subsample=0.7, colsample_bytree=0.6, gamma=0.2, reg_alpha=1, reg_lambda=2, k_num=10, y_var='Recharge RC 50% mm/y', y_predict='R50', aus_file='Australia_grid_0p05_data.csv', seed=42, test_data=False):\n",
    "    start_time = datetime.now()\n",
    "    DataLocation = os.path.join('..', 'data')\n",
    "    os.chdir(DataLocation)\n",
    "\n",
    "    df = pd.read_csv('dat07_u.csv', low_memory=False).sample(frac=1, random_state=seed)\n",
    "    df.dropna(subset=['Rain mm/y', 'koppen_geiger', 'PET mm/y', 'distance_to_coast_km', 'Aridity', 'elevation_mahd', 'wtd_mbgs', 'regolith_depth_mbgs', 'slope_perc', 'clay_perc', 'silt_perc', 'sand_perc', 'soil_class', 'geology', 'ndvi_avg', 'vegex_cat', 'rainfall_seasonality'], inplace=True)\n",
    "    print(f\"nans removed, removed {len(df) - len(df.dropna())}, removed {Decimal(100 * (len(df) - len(df.dropna()))/len(df)).quantize(Decimal('1.0'))}%\")\n",
    "    print(f\"Remaining data has mean Rrc/P ratio: {Decimal(np.nanmean(df['Rrc/P'])).quantize(Decimal('1.00'))}\")\n",
    "\n",
    "    train_params = ['Rain mm/y', 'rainfall_seasonality', 'PET mm/y', 'elevation_mahd', 'distance_to_coast_km', 'ndvi_avg', 'clay_perc', 'soil_class']\n",
    "\n",
    "    aus_X = pd.read_csv(aus_file)[train_params]\n",
    "    random.seed(seed)\n",
    "    random_num = random.randint(0, 1000)\n",
    "\n",
    "    if not test_data:\n",
    "        X = df[train_params]\n",
    "        y = df[y_var]\n",
    "        Xtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=t_size, random_state=random_num)\n",
    "    else:\n",
    "        train_data_file = 'train_data.csv'\n",
    "        train_data = pd.read_csv(train_data_file)\n",
    "        Xtrain = train_data[train_params]\n",
    "        ytrain = train_data[y_var]\n",
    "\n",
    "    xgb = XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, min_child_weight=min_child_weight, subsample=subsample, colsample_bytree=colsample_bytree, gamma=gamma, reg_alpha=reg_alpha, reg_lambda=reg_lambda, random_state=random_num)\n",
    "    xgb.fit(Xtrain, ytrain)\n",
    "\n",
    "    print(f'Training Score: {xgb.score(Xtrain, ytrain):.3f}')\n",
    "\n",
    "    if not test_data:\n",
    "        ypredv = xgb.predict(Xvalid)\n",
    "        y_pred_valid = pd.DataFrame({'y_predict': ypredv, 'y_validation': yvalid})\n",
    "        y_pred_valid['Residual (predicted R - CMB R)'] = y_pred_valid['y_predict'] - y_pred_valid['y_validation']\n",
    "        y_pred_valid['Residual (%)'] = ((y_pred_valid['y_predict'] - y_pred_valid['y_validation']) / y_pred_valid['y_validation']) * 100\n",
    "        print(f'R2 Score: {r2_score(yvalid, ypredv):.3f}')\n",
    "        print(f'RMSE: {mean_squared_error(yvalid, ypredv, squared=False):.3f}')\n",
    "        print(f'MAE: {mean_absolute_error(yvalid, ypredv):.3f}')\n",
    "\n",
    "        scoring = {'r2': 'r2', 'mae': 'neg_mean_absolute_error', 'rmse': 'neg_root_mean_squared_error'}\n",
    "        cv_results = cross_validate(xgb, Xtrain, ytrain, cv=k_num, scoring=scoring, n_jobs=-1)\n",
    "        print(f'k={k_num}')\n",
    "        print(f'R2 Score: {np.mean(cv_results[\"test_r2\"]):.3f}')\n",
    "        print(f'RMSE: {-np.mean(cv_results[\"test_rmse\"]):.1f}')\n",
    "        print(f'MAE: {-np.mean(cv_results[\"test_mae\"]):.1f}')\n",
    "\n",
    "        print('Starting predictions...')\n",
    "        y_pred_aus = pd.DataFrame({'lat': pd.read_csv(aus_file).iloc[:,0], 'lon': pd.read_csv(aus_file).iloc[:,1], y_predict: xgb.predict(aus_X)})\n",
    "        print('Finished prediction... writing values')\n",
    "\n",
    "        y_pred_valid.to_csv(f'model_validation_predictions_errors_50_{n_estimators}n_estimators_lr{learning_rate}_{k_num}fold_out.csv', index=False)\n",
    "        y_pred_aus.to_csv(f'model_predictions_aus_{n_estimators}n_estimators_lr{learning_rate}_{k_num}fold_out.csv', index=False)\n",
    "    print(f'Model took: {(datetime.now() - start_time).total_seconds()/60:.2f} minutes to run')\n",
    "\n",
    "    # Save the trained model to a file if using test data\n",
    "    if test_data:\n",
    "        model_dir = os.path.join('..', 'Trained_models')\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        model_filename = os.path.join(model_dir, f'xgb_model_{n_estimators}n_estimators_lr{learning_rate}.pkl')\n",
    "        joblib.dump(xgb, model_filename)\n",
    "        print(f'Model saved to {model_filename}')\n",
    "\n",
    "def optimize_xgb_model(X, y, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1):\n",
    "    xgb = XGBRegressor()\n",
    "    grid_search = HalvingGridSearchCV(estimator=xgb, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=n_jobs, verbose=1, aggressive_elimination=True)\n",
    "    grid_search.fit(X, y)\n",
    "    print(f'Best parameters found: {grid_search.best_params_}')\n",
    "    print(f'Best score: {grid_search.best_score_}')\n",
    "    return grid_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgb_trees(X, y, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1):\n",
    "    xgb = XGBRegressor()\n",
    "    grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=n_jobs, verbose=1, aggressive_elimination=True)\n",
    "    grid_search.fit(X, y)\n",
    "    print(f'Best parameters found: {grid_search.best_params_}')\n",
    "    print(f'Best score: {grid_search.best_score_}')\n",
    "    #return grid_search.best_estimator_\n",
    "\n",
    "    xgb_param = {\n",
    "        'max_depth': 5,\n",
    "        'learning_rate': 0.1,\n",
    "        'min_child_weight': 1,\n",
    "        'subsasmple': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'gamma': 0\n",
    "    }\n",
    "    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=100, nfold=5,\n",
    "                      metrics='auc', early_stopping_rounds=10, verbose_eval=False)\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nans removed, removed 0, removed 0.0%\n",
      "Remaining data has mean Rrc/P ratio: 0.04\n",
      "Training Score: 0.811\n",
      "Model took: 0.39 minutes to run\n",
      "Model saved to ..\\Trained_models\\xgb_model_350n_estimators_lr0.01.pkl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_xgb_model(test_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 7\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 135\n",
      "max_resources_: 98568\n",
      "aggressive_elimination: True\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1296\n",
      "n_resources: 135\n",
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nefel\\miniconda3\\envs\\AI\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 1\n",
      "n_candidates: 432\n",
      "n_resources: 405\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 144\n",
      "n_resources: 1215\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 48\n",
      "n_resources: 3645\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 16\n",
      "n_resources: 10935\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 6\n",
      "n_resources: 32805\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 2\n",
      "n_resources: 98415\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best parameters found: {'colsample_bytree': 0.8, 'gamma': 0.2, 'learning_rate': 0.1, 'max_depth': 50, 'min_child_weight': 20, 'n_estimators': 50}\n",
      "Best score: -2816.0756165242346\n",
      "Optimized model: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=0.2, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=50, max_leaves=None,\n",
      "             min_child_weight=20, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=None, ...)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 75, 100, 125, 150, 200],\n",
    "    'max_depth': [3, 5, 8, 10],\n",
    "    'learning_rate': [0.1, 0.2, 0.3, 0.4],\n",
    "    'min_child_weight': [5, 10],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9, 1.],\n",
    "    'gamma': [0.2]\n",
    "}\n",
    "\n",
    "df = pd.read_csv('dat07_u.csv', low_memory=False).sample(frac=1, random_state=42)\n",
    "df.dropna(subset=['Rain mm/y', 'koppen_geiger', 'PET mm/y', 'distance_to_coast_km', 'Aridity', 'elevation_mahd', 'wtd_mbgs', 'regolith_depth_mbgs', 'slope_perc', 'clay_perc', 'silt_perc', 'sand_perc', 'soil_class', 'geology', 'ndvi_avg', 'vegex_cat', 'rainfall_seasonality'], inplace=True)\n",
    "\n",
    "train_params = ['Rain mm/y', 'rainfall_seasonality', 'PET mm/y', 'elevation_mahd', 'distance_to_coast_km', 'ndvi_avg', 'clay_perc', 'soil_class']\n",
    "X = df[train_params]\n",
    "y = df['Recharge RC 50% mm/y']\n",
    "\n",
    "best_model = optimize_xgb_model(X, y, param_grid)\n",
    "print(f'Optimized model: {best_model}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = 'train_data.csv'\n",
    "train_data = pd.read_csv(train_data_file)\n",
    "Xtrain = train_data[train_params]\n",
    "ytrain = train_data[y_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nans removed, removed 0, removed 0.0%\n",
      "Remaining data has mean Rrc/P ratio: 0.04\n",
      "Training Score: 0.824\n",
      "Model took: 0.38 minutes to run\n",
      "Model saved to ..\\Trained_models\\xgb_model_50n_estimators_lr0.1.pkl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_xgb_model(gamma=0, n_estimators=50, max_depth=5, learning_rate=0.1, min_child_weight=1, subsample=0.8, colsample_bytree=0.8, test_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random uniform attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb_model(t_size=0.3, n_estimators=50, max_depth=30, learning_rate=0.01, min_child_weight=20, colsample_bytree=0.85, gamma=0.2, k_num=10, y_var='Recharge RC 50% mm/y', y_predict='R50', aus_file='Australia_grid_0p05_data.csv', seed=42, test_data=False):\n",
    "    start_time = datetime.now()\n",
    "    DataLocation = os.path.join('..', 'data')\n",
    "    os.chdir(DataLocation)\n",
    "\n",
    "    df = pd.read_csv('dat07_u.csv', low_memory=False).sample(frac=1, random_state=seed)\n",
    "    df.dropna(subset=['Rain mm/y', 'koppen_geiger', 'PET mm/y', 'distance_to_coast_km', 'Aridity', 'elevation_mahd', 'wtd_mbgs', 'regolith_depth_mbgs', 'slope_perc', 'clay_perc', 'silt_perc', 'sand_perc', 'soil_class', 'geology', 'ndvi_avg', 'vegex_cat', 'rainfall_seasonality'], inplace=True)\n",
    "    print(f\"nans removed, removed {len(df) - len(df.dropna())}, removed {Decimal(100 * (len(df) - len(df.dropna()))/len(df)).quantize(Decimal('1.0'))}%\")\n",
    "    print(f\"Remaining data has mean Rrc/P ratio: {Decimal(np.nanmean(df['Rrc/P'])).quantize(Decimal('1.00'))}\")\n",
    "\n",
    "    train_params = ['Rain mm/y', 'rainfall_seasonality', 'PET mm/y', 'elevation_mahd', 'distance_to_coast_km', 'ndvi_avg', 'clay_perc', 'soil_class']\n",
    "\n",
    "    aus_X = pd.read_csv(aus_file)[train_params]\n",
    "    random.seed(seed)\n",
    "    random_num = random.randint(0, 1000)\n",
    "\n",
    "    if not test_data:\n",
    "        X = df[train_params]\n",
    "        y = df[y_var]\n",
    "        Xtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=t_size, random_state=random_num)\n",
    "    else:\n",
    "        train_data_file = 'train_data.csv'\n",
    "        train_data = pd.read_csv(train_data_file)\n",
    "        Xtrain = train_data[train_params]\n",
    "        ytrain = train_data[y_var]\n",
    "\n",
    "    xgb = XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, min_child_weight=min_child_weight, colsample_bytree=colsample_bytree, gamma=gamma, random_state=random_num)\n",
    "    xgb.fit(Xtrain, ytrain)\n",
    "\n",
    "    print(f'Training Score: {xgb.score(Xtrain, ytrain):.3f}')\n",
    "\n",
    "    if not test_data:\n",
    "        ypredv = xgb.predict(Xvalid)\n",
    "        y_pred_valid = pd.DataFrame({'y_predict': ypredv, 'y_validation': yvalid})\n",
    "        y_pred_valid['Residual (predicted R - CMB R)'] = y_pred_valid['y_predict'] - y_pred_valid['y_validation']\n",
    "        y_pred_valid['Residual (%)'] = ((y_pred_valid['y_predict'] - y_pred_valid['y_validation']) / y_pred_valid['y_validation']) * 100\n",
    "        print(f'R2 Score: {r2_score(yvalid, ypredv):.3f}')\n",
    "        print(f'RMSE: {mean_squared_error(yvalid, ypredv, squared=False):.3f}')\n",
    "        print(f'MAE: {mean_absolute_error(yvalid, ypredv):.3f}')\n",
    "\n",
    "        scoring = {'r2': 'r2', 'mae': 'neg_mean_absolute_error', 'rmse': 'neg_root_mean_squared_error'}\n",
    "        cv_results = cross_validate(xgb, Xtrain, ytrain, cv=k_num, scoring=scoring, n_jobs=-1)\n",
    "        print(f'k={k_num}')\n",
    "        print(f'R2 Score: {np.mean(cv_results[\"test_r2\"]):.3f}')\n",
    "        print(f'RMSE: {-np.mean(cv_results[\"test_rmse\"]):.1f}')\n",
    "        print(f'MAE: {-np.mean(cv_results[\"test_mae\"]):.1f}')\n",
    "\n",
    "        print('Starting predictions...')\n",
    "        y_pred_aus = pd.DataFrame({'lat': pd.read_csv(aus_file).iloc[:,0], 'lon': pd.read_csv(aus_file).iloc[:,1], y_predict: xgb.predict(aus_X)})\n",
    "        print('Finished prediction... writing values')\n",
    "\n",
    "        y_pred_valid.to_csv(f'model_validation_predictions_errors_50_{n_estimators}n_estimators_lr{learning_rate}_{k_num}fold_out.csv', index=False)\n",
    "        y_pred_aus.to_csv(f'model_predictions_aus_{n_estimators}n_estimators_lr{learning_rate}_{k_num}fold_out.csv', index=False)\n",
    "    print(f'Model took: {(datetime.now() - start_time).total_seconds()/60:.2f} minutes to run')\n",
    "\n",
    "    # Save the trained model to a file if using test data\n",
    "    if test_data:\n",
    "        model_dir = os.path.join('..', 'Trained_models')\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        model_filename = os.path.join(model_dir, f'xgb_model_{n_estimators}n_estimators_lr{learning_rate}.pkl')\n",
    "        joblib.dump(xgb, model_filename)\n",
    "        print(f'Model saved to {model_filename}')\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "def optimize_xgb_model(X, y, param_distributions, n_iter=100, cv=5, scoring='neg_mean_squared_error', n_jobs=-1):\n",
    "    xgb = XGBRegressor()\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X, y)\n",
    "    print(f'Best parameters found: {random_search.best_params_}')\n",
    "    print(f'Best score: {random_search.best_score_}')\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_distributions = {\n",
    "    'colsample_bytree': np.random.uniform(0.8, 1.0, 10),\n",
    "    'gamma': np.random.uniform(0.2, 0.6, 10),\n",
    "    'learning_rate': np.random.uniform(0.1, 0.5, 10),\n",
    "    'max_depth': np.random.randint(10, 111, 10),\n",
    "    'min_child_weight': np.random.randint(10, 31, 10),\n",
    "    'n_estimators': np.random.randint(50, 201, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m df[train_params]\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecharge RC 50\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m mm/y\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_xgb_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimized model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 80\u001b[0m, in \u001b[0;36moptimize_xgb_model\u001b[1;34m(X, y, param_distributions, n_iter, cv, scoring, n_jobs)\u001b[0m\n\u001b[0;32m     69\u001b[0m xgb \u001b[38;5;241m=\u001b[39m XGBRegressor()\n\u001b[0;32m     70\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     71\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mxgb,\n\u001b[0;32m     72\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_distributions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     79\u001b[0m )\n\u001b[1;32m---> 80\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_search\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nefel\\miniconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nefel\\miniconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nefel\\miniconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1959\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nefel\\miniconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nefel\\miniconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nefel\\miniconda3\\envs\\AI\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nefel\\miniconda3\\envs\\AI\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nefel\\miniconda3\\envs\\AI\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dat07_u.csv', low_memory=False).sample(frac=1, random_state=42)\n",
    "df.dropna(subset=['Rain mm/y', 'koppen_geiger', 'PET mm/y', 'distance_to_coast_km', 'Aridity', 'elevation_mahd', 'wtd_mbgs', 'regolith_depth_mbgs', 'slope_perc', 'clay_perc', 'silt_perc', 'sand_perc', 'soil_class', 'geology', 'ndvi_avg', 'vegex_cat', 'rainfall_seasonality'], inplace=True)\n",
    "\n",
    "train_params = ['Rain mm/y', 'rainfall_seasonality', 'PET mm/y', 'elevation_mahd', 'distance_to_coast_km', 'ndvi_avg', 'clay_perc', 'soil_class']\n",
    "X = df[train_params]\n",
    "y = df['Recharge RC 50% mm/y']\n",
    "\n",
    "best_model = optimize_xgb_model(X, y, param_distributions)\n",
    "print(f'Optimized model: {best_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nans removed, removed 0, removed 0.0%\n",
      "Remaining data has mean Rrc/P ratio: 0.04\n",
      "Training Score: 0.469\n",
      "Model took: 0.11 minutes to run\n",
      "Model saved to ..\\Trained_models\\xgb_model_50n_estimators_lr0.01.pkl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_xgb_model(test_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters found: {'colsample_bytree': 0.9, 'gamma': 0.2, 'learning_rate': 0.15, 'max_depth': 10, 'min_child_weight': 20, 'n_estimators': 100}\n",
    "Best score: -2832.4503219329135\n",
    "Optimized model: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "             colsample_bylevel=None, colsample_bynode=None,\n",
    "             colsample_bytree=0.9, device=None, early_stopping_rounds=None,\n",
    "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "             gamma=0.2, grow_policy=None, importance_type=None,\n",
    "             interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
    "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
    "             min_child_weight=20, missing=nan, monotone_constraints=None,\n",
    "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
    "             num_parallel_tree=None, random_state=None, ...)\n",
    "\n",
    "nans removed, removed 0, removed 0.0%\n",
    "Remaining data has mean Rrc/P ratio: 0.04\n",
    "Training Score: 0.823\n",
    "Model took: 0.13 minutes to run\n",
    "Model saved to ..\\Trained_models\\xgb_model_100n_estimators_lr0.15.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters found: {'colsample_bytree': 0.85, 'gamma': 0.2, 'learning_rate': 0.1, 'max_depth': 30, 'min_child_weight': 20, 'n_estimators': 50}\n",
    "Best score: -2815.121360563002\n",
    "Optimized model: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "             colsample_bylevel=None, colsample_bynode=None,\n",
    "             colsample_bytree=0.85, device=None, early_stopping_rounds=None,\n",
    "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "             gamma=0.2, grow_policy=None, importance_type=None,\n",
    "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
    "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "             max_delta_step=None, max_depth=30, max_leaves=None,\n",
    "             min_child_weight=20, missing=nan, monotone_constraints=None,\n",
    "             multi_strategy=None, n_estimators=50, n_jobs=None,\n",
    "             num_parallel_tree=None, random_state=None, ...)\n",
    "\n",
    "\n",
    "nans removed, removed 0, removed 0.0%\n",
    "Remaining data has mean Rrc/P ratio: 0.04\n",
    "Training Score: 0.848\n",
    "Model took: 0.15 minutes to run\n",
    "Model saved to ..\\Trained_models\\xgb_model_50n_estimators_lr0.1.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
