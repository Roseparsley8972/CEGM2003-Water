{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Channel CNN with 2 branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "# Directories\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nefel\\AppData\\Local\\Temp\\ipykernel_18396\\2508953001.py:23: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  clor = pd.read_csv(\"../Data/dat07_u.csv\")\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Raster Images\n",
    "def load_raster_data(filepaths):\n",
    "    rasters = []\n",
    "    for filepath in filepaths:\n",
    "        with rasterio.open(filepath) as src:\n",
    "            rasters.append(src.read(1))  # Load the first band\n",
    "    return np.stack(rasters, axis=-1), src.transform\n",
    "\n",
    "# Function to generate file paths for n raster images\n",
    "def generate_raster_paths(base_path, img_names):\n",
    "    return [os.path.join(base_path, img_name) for img_name in img_names]\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Set the names of the input images!!\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# List of raster image filenames\n",
    "img_names = [\"rainfall_raster_bound.tif\", \"clay_raster_bound.tif\"]  # Add more as needed\n",
    "data_paths = generate_raster_paths(os.path.join(\"..\", \"CNN_from_R\", \"images_for_CNN\"), img_names)\n",
    "in_ims_stacked, transform = load_raster_data(data_paths)\n",
    "\n",
    "# 2. Load the recharge rate data\n",
    "clor = pd.read_csv(\"../Data/dat07_u.csv\")\n",
    "quant = \"Recharge RC 50% mm/y\" \n",
    "\n",
    "# Drop NaNs for essential columns\n",
    "clor = clor.dropna(subset=[\"lat\", \"lon\", quant])\n",
    "\n",
    "# Convert clor to a GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(clor['lon'], clor['lat'])]\n",
    "clor_gdf = gpd.GeoDataFrame(clor, geometry=geometry, crs=\"EPSG:3577\")  # WGS84 (lat/lon)\n",
    "\n",
    "# Extract the reprojected coordinates\n",
    "clor_gdf['x'] = clor_gdf.geometry.x\n",
    "clor_gdf['y'] = clor_gdf.geometry.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Extract Sample-Centered Terrain Images\n",
    "def extract_sample_centered_images(clor_gdf, in_ims_stacked, transform, size=32):\n",
    "    rows, cols, num_layers = in_ims_stacked.shape\n",
    "    half_size = size // 2\n",
    "    terrain_images = []\n",
    "\n",
    "    for _, row in clor_gdf.iterrows():\n",
    "        x, y = row[\"x\"], row[\"y\"]\n",
    "        col, row = ~transform * (x, y)\n",
    "        col, row = int(col), int(row)\n",
    "        \n",
    "        if 0 <= row-half_size < rows and 0 <= col-half_size < cols:\n",
    "            img = in_ims_stacked[row-half_size:row+half_size, col-half_size:col+half_size, :]\n",
    "            if img.shape == (size, size, num_layers):\n",
    "                terrain_images.append(img)\n",
    "            else:\n",
    "                terrain_images.append(np.zeros((size, size, num_layers)))  # Pad with zeros if out of bounds\n",
    "        else:\n",
    "            terrain_images.append(np.zeros((size, size, num_layers)))  # Completely out of bounds\n",
    "\n",
    "    return np.array(terrain_images)\n",
    "\n",
    "# Use the updated coordinates\n",
    "imgs_ann = extract_sample_centered_images(clor_gdf, in_ims_stacked, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract Sample-Centered Terrain Images WITH NORMALIZATION\n",
    "\n",
    "def extract_sample_centered_images(clor_gdf, in_ims_stacked, transform, size=32):\n",
    "    rows, cols, num_layers = in_ims_stacked.shape\n",
    "    half_size = size // 2\n",
    "    terrain_images = []\n",
    "\n",
    "    for _, row in clor_gdf.iterrows():\n",
    "        x, y = row[\"x\"], row[\"y\"]\n",
    "        col, row = ~transform * (x, y)\n",
    "        col, row = int(col), int(row)\n",
    "        \n",
    "        if 0 <= row-half_size < rows and 0 <= col-half_size < cols:\n",
    "            img = in_ims_stacked[row-half_size:row+half_size, col-half_size:col+half_size, :]\n",
    "            if img.shape == (size, size, num_layers):\n",
    "                # Normalize the image\n",
    "                center_value = img[half_size, half_size, :]\n",
    "                img = img - center_value  # Center the image\n",
    "                img_std = img.std() if img.std() != 0 else 1  # Avoid division by zero\n",
    "                img = img / img_std  # Scale by standard deviation\n",
    "                terrain_images.append(img)\n",
    "            else:\n",
    "                terrain_images.append(np.zeros((size, size, num_layers)))  # Pad with zeros if out of bounds\n",
    "        else:\n",
    "            terrain_images.append(np.zeros((size, size, num_layers)))  # Completely out of bounds\n",
    "\n",
    "    return np.array(terrain_images)\n",
    "\n",
    "imgs_ann = extract_sample_centered_images(clor_gdf, in_ims_stacked, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ..\\CNN_from_R\\images_for_CNN\\rainfall_raster_bound.tif - Pixel size: 0.05 x 0.05\n",
      "File: ..\\CNN_from_R\\images_for_CNN\\clay_raster_bound.tif - Pixel size: 0.05 x 0.05\n"
     ]
    }
   ],
   "source": [
    "for path in data_paths:\n",
    "    with rasterio.open(path) as src:\n",
    "        transform = src.transform\n",
    "        pixel_width = transform[0]   # `a` value: pixel size in x-direction\n",
    "        pixel_height = -transform[4] # `e` value (negated because it's typically negative)\n",
    "        print(f\"File: {path} - Pixel size: {pixel_width} x {pixel_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the centered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 5\n",
    "\n",
    "# Select random images or specific indices\n",
    "indices = np.random.choice(imgs_ann.shape[0], n, replace=False)\n",
    "\n",
    "# Plot the images\n",
    "fig, axs = plt.subplots(1, n, figsize=(15, 5))\n",
    "for i, idx in enumerate(indices):\n",
    "    axs[i].imshow(imgs_ann[idx,:,:,1], cmap=\"binary\")\n",
    "    axs[i].set_title(f\"Image {idx}\")\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the location input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Select the same features in loc as the images selected in the cell above\n",
    "# -----------------------------------------------------------------------\n",
    "recharge_data = clor[['lat','lon','Rain mm/y', 'Recharge RC 50% mm/y', 'rainfall_seasonality', 'PET mm/y', 'elevation_mahd', 'distance_to_coast_km', 'ndvi_avg', 'clay_perc', 'soil_class']]\n",
    "loc = recharge_data[['lat','lon','Rain mm/y', 'clay_perc']]\n",
    "#loc = recharge_data[['lat','lon','Rain mm/y', 'rainfall_seasonality', 'PET mm/y', 'elevation_mahd', 'distance_to_coast_km', 'ndvi_avg', 'clay_perc', 'soil_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(img_names)\n",
    "if (loc.shape[1] - 2) != len(img_names):\n",
    "    print('WARNING: THE FEATURE SIZES ARE DIFFERENT FOR THE TWO BRANCHES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedim = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in 10 folds\n",
    "np.random.seed(321)\n",
    "fold_size = recharge_data.shape[0] // 10\n",
    "\n",
    "# Shuffles the indices of the dataset\n",
    "indices = np.arange(recharge_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split indices into test, validation, and train sets\n",
    "test = indices[:fold_size]\n",
    "val = indices[fold_size:2*fold_size]\n",
    "train = indices[2*fold_size:]\n",
    "\n",
    "x_train_imgs, x_train_loc = imgs_ann[train], loc.iloc[train].to_numpy()\n",
    "x_val_imgs, x_val_loc = imgs_ann[val], loc.iloc[val].to_numpy()\n",
    "x_test_imgs, x_test_loc = imgs_ann[test], loc.iloc[test].to_numpy()\n",
    "\n",
    "# Take the log of the recharge rate to make the distribution less skewed and set as target\n",
    "y_train = np.log(recharge_data['Recharge RC 50% mm/y'].astype(float)[train])\n",
    "y_val = np.log(recharge_data['Recharge RC 50% mm/y'].astype(float)[val])\n",
    "y_test = np.log(recharge_data['Recharge RC 50% mm/y'].astype(float)[test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataNormalizer:\n",
    "    def __init__(self, train_data):\n",
    "        # Calculate mean and standard deviation from training data\n",
    "        self.mean = train_data.mean(axis=0)\n",
    "        self.std = train_data.std(axis=0)\n",
    "\n",
    "    def normalize(self, data):\n",
    "        # Normalize data using the mean and std from the training data\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "# Create a normalizer using the training data\n",
    "normalizer = DataNormalizer(x_train_loc)\n",
    "\n",
    "# Normalize the validation and test data using the same normalizer\n",
    "x_train_loc_norm = normalizer.normalize(x_train_loc)\n",
    "x_val_loc_norm = normalizer.normalize(x_val_loc)\n",
    "x_test_loc_norm = normalizer.normalize(x_test_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [x_train_imgs, x_train_loc_norm]\n",
    "x_val = [x_val_imgs, x_val_loc_norm]\n",
    "x_test = [x_test_imgs, x_test_loc_norm]\n",
    "\n",
    "# Reshape the image data\n",
    "x_train[0] = x_train[0].reshape((x_train[0].shape[0], imagedim, imagedim, num_features))\n",
    "x_val[0] = x_val[0].reshape((x_val[0].shape[0], imagedim, imagedim, num_features))\n",
    "x_test[0] = x_test[0].reshape((x_test[0].shape[0], imagedim, imagedim, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nefel\\miniconda3\\envs\\keras\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nefel\\miniconda3\\envs\\keras\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nefel\\miniconda3\\envs\\keras\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use neural network to learn relationship between terrain features and geochemistry ####\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, SpatialDropout2D, GlobalAveragePooling2D, Flatten, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.initializers import HeNormal, Zeros\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All `outputs` values must be KerasTensors. Received: outputs=<Lambda name=lambda_7, built=False> including invalid value <Lambda name=lambda_7, built=False> of type <class 'keras.src.layers.core.lambda_layer.Lambda'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 64\u001b[0m\n\u001b[0;32m     57\u001b[0m distribution_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLambda(\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: tfd\u001b[38;5;241m.\u001b[39mNormal(\n\u001b[0;32m     59\u001b[0m         loc\u001b[38;5;241m=\u001b[39mx[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# Equivalent to x[, 1, drop = FALSE] in R\u001b[39;00m\n\u001b[0;32m     60\u001b[0m         scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m x[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# Equivalent to x[, 2, drop = FALSE] in R\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     )\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Remove the model if it exists (not needed in Python as we can just overwrite)\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mconv_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliary_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistribution_layer\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnegloglik0\u001b[39m(y, model):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlog_prob(y)\n",
      "File \u001b[1;32mc:\\Users\\nefel\\miniconda3\\envs\\keras\\Lib\\site-packages\\keras\\src\\utils\\tracking.py:26\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DotNotTrackScope():\n\u001b[1;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nefel\\miniconda3\\envs\\keras\\Lib\\site-packages\\keras\\src\\models\\functional.py:126\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flat_outputs:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, backend\u001b[38;5;241m.\u001b[39mKerasTensor):\n\u001b[1;32m--> 126\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    127\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll `outputs` values must be KerasTensors. Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m including invalid value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m         )\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_input_keras_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_inputs):\n\u001b[0;32m    133\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m clone_graph_nodes(inputs, outputs)\n",
      "\u001b[1;31mValueError\u001b[0m: All `outputs` values must be KerasTensors. Received: outputs=<Lambda name=lambda_7, built=False> including invalid value <Lambda name=lambda_7, built=False> of type <class 'keras.src.layers.core.lambda_layer.Lambda'>"
     ]
    }
   ],
   "source": [
    "\n",
    "# We assume that the number of features is the same in both branches\n",
    "input_features = num_features\n",
    "imagedim = 32\n",
    "\n",
    "# Defining Model ----------------------------------------------------------\n",
    "time = tf.timestamp()\n",
    "\n",
    "dropratespat = 0.5  # match what model was trained with\n",
    "dropratedense = 0.2  # match what model was trained with\n",
    "\n",
    "kernel_ini = HeNormal()\n",
    "bias_ini = Zeros()\n",
    "\n",
    "# Convolutional stack:\n",
    "conv_input = Input(shape=(imagedim, imagedim, input_features), name='conv_input')\n",
    "\n",
    "conv_output = Conv2D(128, (3, 3), dilation_rate=1, strides=3, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(conv_input)\n",
    "conv_output = Activation(\"relu\")(conv_output)\n",
    "conv_output = SpatialDropout2D(rate=dropratespat)(conv_output)\n",
    "\n",
    "conv_output = Conv2D(128, (3, 3), dilation_rate=1, strides=1, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(conv_output)\n",
    "conv_output = Activation(\"relu\")(conv_output)\n",
    "conv_output = SpatialDropout2D(rate=dropratespat)(conv_output)\n",
    "\n",
    "conv_output = Conv2D(128, (3, 3), dilation_rate=1, strides=1, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(conv_output)\n",
    "conv_output = Activation(\"relu\")(conv_output)\n",
    "conv_output = SpatialDropout2D(rate=dropratespat)(conv_output)\n",
    "\n",
    "conv_output = Conv2D(128, (3, 3), dilation_rate=1, strides=1, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(conv_output)\n",
    "conv_output = Activation(\"relu\")(conv_output)\n",
    "conv_output = SpatialDropout2D(rate=dropratespat)(conv_output)\n",
    "\n",
    "conv_output = GlobalAveragePooling2D()(conv_output)\n",
    "conv_output = Flatten()(conv_output)\n",
    "\n",
    "# Auxiliary input:\n",
    "auxiliary_input = Input(shape=(input_features+2,), name='aux_input')\n",
    "\n",
    "auxiliary_output = Dense(1920, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(auxiliary_input)\n",
    "auxiliary_output = Activation(\"relu\")(auxiliary_output)\n",
    "auxiliary_output = Dropout(rate=dropratedense)(auxiliary_output)\n",
    "auxiliary_output = Flatten()(auxiliary_output)\n",
    "\n",
    "# Main output:\n",
    "main_output = Concatenate()([conv_output, auxiliary_output])\n",
    "main_output = Dense(1024, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(main_output)\n",
    "main_output = Activation(\"relu\")(main_output)\n",
    "main_output = Dropout(rate=dropratedense)(main_output)\n",
    "main_output = Dense(256, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(main_output)\n",
    "main_output = Activation(\"relu\")(main_output)\n",
    "main_output = Dropout(rate=dropratedense)(main_output)\n",
    "main_output = Dense(units=2, activation=\"linear\", name=\"dist_param\", kernel_initializer=kernel_ini, bias_initializer=bias_ini)(main_output)\n",
    "#main_output = tf.keras.layers.Lambda(lambda x: tf.random.normal(shape=tf.shape(x[:, 0:1]), mean=x[:, 0:1], stddev=1e-3 + tf.nn.softplus(0.1 * x[:, 1:2])))(main_output)\n",
    "#main_output = tf.keras.layers.Lambda(\n",
    "#    lambda x: tf.concat([x[:, 0:1], tf.math.softplus(x[:, 1:2])], axis=1)\n",
    "#)(main_output)\n",
    "distribution_layer = tf.keras.layers.Lambda(\n",
    "    lambda x: tfd.Normal(\n",
    "        loc=x[:, 0:1],  # Equivalent to x[, 1, drop = FALSE] in R\n",
    "        scale=1e-3 + tf.nn.softplus(0.1 * x[:, 1:2])  # Equivalent to x[, 2, drop = FALSE] in R\n",
    "    )\n",
    ")\n",
    "# Remove the model if it exists (not needed in Python as we can just overwrite)\n",
    "model = tf.keras.Model(\n",
    "    inputs=[conv_input, auxiliary_input], \n",
    "    outputs=distribution_layer\n",
    ")\n",
    "\n",
    "#def negloglik0(y, model):\n",
    "#    return -model.log_prob(y)\n",
    "\n",
    "# Define the negative log-likelihood function\n",
    "def negloglik(y_true, y_pred):\n",
    "    # Assuming y_pred is a distribution, e.g., Normal distribution\n",
    "    dist = tfd.Normal(loc=y_pred[:, 0], scale=tf.math.softplus(y_pred[:, 1]))\n",
    "    return -tf.reduce_mean(dist.log_prob(y_true))\n",
    "\n",
    "# Define the negative log-likelihood function\n",
    "#def negloglik(y, model):\n",
    "    # return -model.log_prob(y)\n",
    "\n",
    "# Define the optimizer\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(\n",
    "#    loss=negloglik,\n",
    "#    optimizer=opt\n",
    "#)\n",
    "\n",
    "# Print the summary of the model\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# Define the model\n",
    "#model = models.Model(inputs=[i, auxiliary_input], outputs=main_output)\n",
    "\n",
    "# Define the negative log-likelihood function\n",
    "#def negloglik(y_true, y_pred):\n",
    "#    # Assuming y_pred is a distribution, e.g., Normal distribution\n",
    "#    dist = tfd.Normal(loc=y_pred[:, 0], scale=tf.math.softplus(y_pred[:, 1]))\n",
    "#    return -tf.reduce_mean(dist.log_prob(y_true))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "              loss=negloglik)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for saving the best weights\n",
    "class SaveBestModel(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_best_metric='val_loss', this_max=False):\n",
    "        self.save_best_metric = save_best_metric\n",
    "        self.max = this_max\n",
    "        if this_max:\n",
    "            self.best = float('-inf')\n",
    "        else:\n",
    "            self.best = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metric_value = logs[self.save_best_metric]\n",
    "        if self.max:\n",
    "            if metric_value > self.best:\n",
    "                self.best = metric_value\n",
    "                self.best_weights = self.model.get_weights()\n",
    "\n",
    "        else:\n",
    "            if metric_value < self.best:\n",
    "                self.best = metric_value\n",
    "                self.best_weights= self.model.get_weights()\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Training ----------------------------------------------------------------\n",
    "# Set batch size and number of epochs\n",
    "batch_size = 10\n",
    "epochs = 30\n",
    "\n",
    "# Train the model with the specified parameters\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[save_best_model]\n",
    "    # Uncomment the following lines to use callbacks for early stopping and model checkpointing\n",
    "    # callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200),\n",
    "    #            tf.keras.callbacks.ModelCheckpoint(monitor='val_loss', save_best_only=True, save_weights_only=True,\n",
    "    #                                              filepath=os.path.join(os.getcwd(), \"models/modelweights.hdf5\"))]\n",
    ")\n",
    "\n",
    "# Find the minimum validation loss\n",
    "min_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "# Note: Python does not have a direct equivalent of R's Sys.time() - time\n",
    "# You would need to manually track the start and end time using datetime or time module\n",
    "\n",
    "# Find the epoch with the minimum validation loss\n",
    "best_epoch = history.history['val_loss'].index(min_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading weights of best epoch\n",
    "model.set_weights(save_best_model.best_weights)\n",
    "# Create a DataFrame for holdout predictions and observations\n",
    "holdout = pd.DataFrame({'obs': y_test, 'preds': model.predict(x_test)[:, 0]})\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate and print R squared and RMSE\n",
    "print(f\"R squared = {round(r2_score(holdout['obs'], holdout['preds']), 3)}\")\n",
    "print(f\"RMSE = {round(np.sqrt(np.mean((holdout['preds'] - holdout['obs'])**2)), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleepblop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nef: I have worked until here, the rest is the older code. The code I replaced the above code with is at the very end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ----------------------------------------------------------------\n",
    "# Set batch size and number of epochs\n",
    "batch_size = 8\n",
    "epochs = 1\n",
    "\n",
    "# Train the model with the specified parameters\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    "    # Uncomment the following lines to use callbacks for early stopping and model checkpointing\n",
    "    # callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200),\n",
    "    #            tf.keras.callbacks.ModelCheckpoint(monitor='val_loss', save_best_only=True, save_weights_only=True,\n",
    "    #                                              filepath=os.path.join(os.getcwd(), \"models/modelweights.hdf5\"))]\n",
    ")\n",
    "\n",
    "# Find the minimum validation loss\n",
    "min_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "# Note: Python does not have a direct equivalent of R's Sys.time() - time\n",
    "# You would need to manually track the start and end time using datetime or time module\n",
    "\n",
    "# Find the epoch with the minimum validation loss\n",
    "best_epoch = history.history['val_loss'].index(min_val_loss)\n",
    "\n",
    "# Save and load model weights\n",
    "model.save_weights(os.path.join(os.getcwd(), \"models/modelweights.weights.h5\"))\n",
    "model.load_weights(os.path.join(os.getcwd(), \"models/modelweights.weights.h5\"))\n",
    "\n",
    "# Create a new model for predictions using the 'dist_param' layer\n",
    "meanmodel = tf.keras.Model(\n",
    "    inputs=model.input,\n",
    "    outputs=model.get_layer(\"dist_param\").output\n",
    ")\n",
    "\n",
    "# Save and load the mean model\n",
    "meanmodel.save(os.path.join(os.getcwd(), \"models/meanmodel.h5\"))\n",
    "meanmodel = tf.keras.models.load_model(os.path.join(os.getcwd(), \"models/meanmodel.h5\"))\n",
    "\n",
    "# Create a DataFrame for holdout predictions and observations\n",
    "holdout = pd.DataFrame({'obs': y_test, 'preds': meanmodel.predict(x_test)[:, 0]})\n",
    "\n",
    "# Calculate and print R squared and RMSE\n",
    "print(f\"R squared = {round(np.corrcoef(holdout['preds'], holdout['obs'])[0, 1]**2, 3)}\")\n",
    "print(f\"RMSE = {round(np.sqrt(np.mean((holdout['preds'] - holdout['obs'])**2)), 3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " conv_input (InputLayer)        [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 10, 10, 128)  3584        ['conv_input[0][0]']             \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 10, 10, 128)  0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_12 (SpatialD  (None, 10, 10, 128)  0          ['activation_21[0][0]']          \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 8, 8, 128)    147584      ['spatial_dropout2d_12[0][0]']   \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 8, 8, 128)    0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_13 (SpatialD  (None, 8, 8, 128)   0           ['activation_22[0][0]']          \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 6, 6, 128)    147584      ['spatial_dropout2d_13[0][0]']   \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 6, 6, 128)    0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_14 (SpatialD  (None, 6, 6, 128)   0           ['activation_23[0][0]']          \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 4, 4, 128)    147584      ['spatial_dropout2d_14[0][0]']   \n",
      "                                                                                                  \n",
      " aux_input (InputLayer)         [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 4, 4, 128)    0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1920)         13440       ['aux_input[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_15 (SpatialD  (None, 4, 4, 128)   0           ['activation_24[0][0]']          \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 1920)         0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 128)         0           ['spatial_dropout2d_15[0][0]']   \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 1920)         0           ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 128)          0           ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 1920)         0           ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 2048)         0           ['flatten_6[0][0]',              \n",
      "                                                                  'flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1024)         2098176     ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 1024)         0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 1024)         0           ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 256)          262400      ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 256)          0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 256)          0           ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " dist_param (Dense)             (None, 2)            514         ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 1)            0           ['dist_param[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,820,866\n",
      "Trainable params: 2,820,866\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = models.Model(inputs=[i, auxiliary_input], outputs=main_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "              loss='mean_squared_error',  # Adjust loss function as needed\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleepblop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification of the original code to make it into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use neural network to learn relationship between terrain features and geochemistry ####\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, SpatialDropout2D, GlobalAveragePooling2D, Flatten, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.initializers import HeNormal, Zeros\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "np.random.seed(321)\n",
    "fold_size = gchem.shape[0] // 10\n",
    "test = np.random.choice(gchem.shape[0], fold_size, replace=False)\n",
    "val = np.random.choice(np.setdiff1d(np.arange(gchem.shape[0]), test), fold_size, replace=False)\n",
    "train = np.setdiff1d(np.arange(gchem.shape[0]), np.concatenate((test, val)))\n",
    "\n",
    "# Data Preparation --------------------------------------------------------\n",
    "\n",
    "x_train = [imgs_ann[train, :, :], loc_ann[train, :]]\n",
    "x_val = [imgs_ann[val, :, :], loc_ann[val, :]]\n",
    "x_test = [imgs_ann[test, :, :], loc_ann[test, :]]\n",
    "\n",
    "x_train[0] = x_train[0].reshape((x_train[0].shape[0], imagedim, imagedim, 1))\n",
    "x_val[0] = x_val[0].reshape((x_val[0].shape[0], imagedim, imagedim, 1))\n",
    "x_test[0] = x_test[0].reshape((x_test[0].shape[0], imagedim, imagedim, 1))\n",
    "\n",
    "if logtrans:\n",
    "    y_train = np.log(gchem[:, elem].astype(float))[train]\n",
    "    y_val = np.log(gchem[:, elem].astype(float))[val]\n",
    "    y_test = np.log(gchem[:, elem].astype(float))[test]\n",
    "else:\n",
    "    y_train = gchem[:, elem].astype(float)[train]\n",
    "    y_val = gchem[:, elem].astype(float)[val]\n",
    "    y_test = gchem[:, elem].astype(float)[test]\n",
    "\n",
    "# Defining Model ----------------------------------------------------------\n",
    "time = tf.timestamp()\n",
    "\n",
    "dropratespat = 0.5  # match what model was trained with\n",
    "dropratedense = 0.2  # match what model was trained with\n",
    "\n",
    "kernel_ini = HeNormal()\n",
    "bias_ini = Zeros()\n",
    "\n",
    "# Convolutional stack:\n",
    "conv_input = Input(shape=(imagedim, imagedim, 1), name='conv_input')\n",
    "\n",
    "conv_output = Conv2D(128, (3, 3), dilation_rate=1, strides=3, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(conv_input)\n",
    "conv_output = Activation(\"relu\")(conv_output)\n",
    "conv_output = SpatialDropout2D(rate=dropratespat)(conv_output)\n",
    "\n",
    "conv_output = Conv2D(128, (3, 3), dilation_rate=1, strides=1, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(conv_output)\n",
    "conv_output = Activation(\"relu\")(conv_output)\n",
    "conv_output = SpatialDropout2D(rate=dropratespat)(conv_output)\n",
    "\n",
    "conv_output = Conv2D(128, (3, 3), dilation_rate=1, strides=1, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(conv_output)\n",
    "conv_output = Activation(\"relu\")(conv_output)\n",
    "conv_output = SpatialDropout2D(rate=dropratespat)(conv_output)\n",
    "\n",
    "conv_output = Conv2D(128, (3, 3), dilation_rate=1, strides=1, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(conv_output)\n",
    "conv_output = Activation(\"relu\")(conv_output)\n",
    "conv_output = SpatialDropout2D(rate=dropratespat)(conv_output)\n",
    "\n",
    "conv_output = GlobalAveragePooling2D()(conv_output)\n",
    "conv_output = Flatten()(conv_output)\n",
    "\n",
    "# Auxiliary input:\n",
    "auxiliary_input = Input(shape=(3,), name='aux_input')\n",
    "\n",
    "auxiliary_output = Dense(1920, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(auxiliary_input)\n",
    "auxiliary_output = Activation(\"relu\")(auxiliary_output)\n",
    "auxiliary_output = Dropout(rate=dropratedense)(auxiliary_output)\n",
    "auxiliary_output = Flatten()(auxiliary_output)\n",
    "\n",
    "# Main output:\n",
    "main_output = Concatenate()([conv_output, auxiliary_output])\n",
    "main_output = Dense(1024, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(main_output)\n",
    "main_output = Activation(\"relu\")(main_output)\n",
    "main_output = Dropout(rate=dropratedense)(main_output)\n",
    "main_output = Dense(256, kernel_initializer=kernel_ini, bias_initializer=bias_ini)(main_output)\n",
    "main_output = Activation(\"relu\")(main_output)\n",
    "main_output = Dropout(rate=dropratedense)(main_output)\n",
    "main_output = Dense(units=2, activation=\"linear\", name=\"dist_param\", kernel_initializer=kernel_ini, bias_initializer=bias_ini)(main_output)\n",
    "main_output = tf.keras.layers.Lambda(lambda x: tfd.Normal(loc=x[:, 0:1], scale=1e-3 + tf.nn.softplus(0.1 * x[:, 1:2])))(main_output)\n",
    "\n",
    "# Remove the model if it exists (not needed in Python as we can just overwrite)\n",
    "model = tf.keras.Model(\n",
    "    inputs=[conv_input, auxiliary_input], \n",
    "    outputs=main_output\n",
    ")\n",
    "\n",
    "# Print the summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Define the negative log-likelihood function\n",
    "def negloglik(y, model):\n",
    "    return -model.log_prob(y)\n",
    "\n",
    "# Define the optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=negloglik,\n",
    "    optimizer=opt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " conv_input (InputLayer)        [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 10, 10, 128)  1280        ['conv_input[0][0]']             \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 10, 10, 128)  0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_24 (SpatialD  (None, 10, 10, 128)  0          ['activation_42[0][0]']          \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 8, 8, 128)    147584      ['spatial_dropout2d_24[0][0]']   \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 8, 8, 128)    0           ['conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_25 (SpatialD  (None, 8, 8, 128)   0           ['activation_43[0][0]']          \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 6, 6, 128)    147584      ['spatial_dropout2d_25[0][0]']   \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 6, 6, 128)    0           ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_26 (SpatialD  (None, 6, 6, 128)   0           ['activation_44[0][0]']          \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 4, 4, 128)    147584      ['spatial_dropout2d_26[0][0]']   \n",
      "                                                                                                  \n",
      " aux_input (InputLayer)         [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 4, 4, 128)    0           ['conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 1920)         7680        ['aux_input[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_27 (SpatialD  (None, 4, 4, 128)   0           ['activation_45[0][0]']          \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 1920)         0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling2d_6 (Gl  (None, 128)         0           ['spatial_dropout2d_27[0][0]']   \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 1920)         0           ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_12 (Flatten)           (None, 128)          0           ['global_average_pooling2d_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " flatten_13 (Flatten)           (None, 1920)         0           ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 2048)         0           ['flatten_12[0][0]',             \n",
      "                                                                  'flatten_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 1024)         2098176     ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 1024)         0           ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 1024)         0           ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 256)          262400      ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 256)          0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 256)          0           ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " dist_param (Dense)             (None, 2)            514         ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 1)            0           ['dist_param[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,812,802\n",
      "Trainable params: 2,812,802\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = models.Model(inputs=[conv_input, auxiliary_input], outputs=main_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "              loss='mean_squared_error',  # Adjust loss function as needed\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7886/7886 - 102s - 13ms/step - accuracy: 0.0000e+00 - loss: 3019.5205 - val_accuracy: 0.0000e+00 - val_loss: 3567.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m308/308\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "R squared = 0.71\n",
      "RMSE = 56.46\n"
     ]
    }
   ],
   "source": [
    "# Training ----------------------------------------------------------------\n",
    "# Set batch size and number of epochs\n",
    "batch_size = 10\n",
    "epochs = 1\n",
    "\n",
    "# Train the model with the specified parameters\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    "    # Uncomment the following lines to use callbacks for early stopping and model checkpointing\n",
    "    # callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200),\n",
    "    #            tf.keras.callbacks.ModelCheckpoint(monitor='val_loss', save_best_only=True, save_weights_only=True,\n",
    "    #                                              filepath=os.path.join(os.getcwd(), \"models/modelweights.hdf5\"))]\n",
    ")\n",
    "\n",
    "# Find the minimum validation loss\n",
    "min_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "# Note: Python does not have a direct equivalent of R's Sys.time() - time\n",
    "# You would need to manually track the start and end time using datetime or time module\n",
    "\n",
    "# Find the epoch with the minimum validation loss\n",
    "best_epoch = history.history['val_loss'].index(min_val_loss)\n",
    "\n",
    "# Save and load model weights\n",
    "model.save_weights(os.path.join(os.getcwd(), \"models/modelweights.weights.h5\"))\n",
    "model.load_weights(os.path.join(os.getcwd(), \"models/modelweights.weights.h5\"))\n",
    "\n",
    "# Create a new model for predictions using the 'dist_param' layer\n",
    "meanmodel = tf.keras.Model(\n",
    "    inputs=model.input,\n",
    "    outputs=model.get_layer(\"dist_param\").output\n",
    ")\n",
    "\n",
    "# Save and load the mean model\n",
    "meanmodel.save(os.path.join(os.getcwd(), \"models/meanmodel.h5\"))\n",
    "meanmodel = tf.keras.models.load_model(os.path.join(os.getcwd(), \"models/meanmodel.h5\"))\n",
    "\n",
    "# Create a DataFrame for holdout predictions and observations\n",
    "holdout = pd.DataFrame({'obs': y_test, 'preds': meanmodel.predict(x_test)[:, 0]})\n",
    "\n",
    "# Calculate and print R squared and RMSE\n",
    "print(f\"R squared = {round(np.corrcoef(holdout['preds'], holdout['obs'])[0, 1]**2, 3)}\")\n",
    "print(f\"RMSE = {round(np.sqrt(np.mean((holdout['preds'] - holdout['obs'])**2)), 3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Score: 0.710\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Calculate and print R score using numpy's corrcoef\n",
    "try:\n",
    "\tr2_score = np.corrcoef(holdout['preds'], holdout['obs'])[0, 1]**2\n",
    "\tprint(f\"R Score: {r2_score:.3f}\")\n",
    "except Exception as e:\n",
    "\tprint(f\"Error calculating R score: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot observed vs predicted values\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      6\u001b[0m sns\u001b[38;5;241m.\u001b[39mscatterplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mholdout, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joept\\OneDrive\\Bureaublad\\CEGM2003\\Project\\.venv\\lib\\site-packages\\seaborn\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelational\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joept\\OneDrive\\Bureaublad\\CEGM2003\\Project\\.venv\\lib\\site-packages\\seaborn\\relational.py:21\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     adjust_legend_subtitles,\n\u001b[0;32m     15\u001b[0m     _default_color,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     _scatter_legend_artist,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m groupby_apply_include_groups\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_statistics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EstimateAggregator, WeightedAggregator\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docstrings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "File \u001b[1;32mc:\\Users\\joept\\OneDrive\\Bureaublad\\CEGM2003\\Project\\.venv\\lib\\site-packages\\seaborn\\_statistics.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde\n\u001b[0;32m     33\u001b[0m     _no_scipy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\joept\\OneDrive\\Bureaublad\\CEGM2003\\Project\\.venv\\lib\\site-packages\\scipy\\stats\\__init__.py:485\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joept\\OneDrive\\Bureaublad\\CEGM2003\\Project\\.venv\\lib\\site-packages\\scipy\\stats\\_stats_py.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _measurements\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, MapWrapper,\n\u001b[0;32m     42\u001b[0m                               rng_integers, _rename_parameter, _contains_nan)\n",
      "File \u001b[1;32mc:\\Users\\joept\\OneDrive\\Bureaublad\\CEGM2003\\Project\\.venv\\lib\\site-packages\\scipy\\spatial\\__init__.py:105\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joept\\OneDrive\\Bureaublad\\CEGM2003\\Project\\.venv\\lib\\site-packages\\scipy\\spatial\\_kdtree.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright Anne M. Archibald 2008\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Released under the scipy license\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32m_ckdtree.pyx:1\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot observed vs predicted values\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x='obs', y='preds', data=holdout, alpha=0.1)\n",
    "plt.plot([holdout['obs'].min(), holdout['obs'].max()], [holdout['obs'].min(), holdout['obs'].max()], 'k--')\n",
    "plt.xlabel(f\"Observed log({elem})\" if logtrans else f\"observed {elem}\")\n",
    "plt.ylabel(f\"Predicted log({elem})\" if logtrans else f\"predicted {elem}\")\n",
    "plt.title(f\"R = {round(np.corrcoef(holdout['preds'], holdout['obs'])[0, 1]**2, 2)}     RMSE = {round(np.sqrt(np.mean((holdout['preds'] - holdout['obs'])**2)), 2)}\")\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.savefig(f\"plots/{elem}_mean_holdout_{round(np.corrcoef(holdout['preds'], holdout['obs'])[0, 1]**2, 2)*100}.png\", dpi=300)\n",
    "\n",
    "# Prepare training history data for plotting\n",
    "trainhist = pd.DataFrame(history.history)[['loss', 'val_loss']]\n",
    "trainhist.columns = ['training', 'testing']\n",
    "trainhist['epoch'] = range(1, len(trainhist) + 1)\n",
    "\n",
    "# Melt the training history data for plotting\n",
    "trainhist_melted = trainhist.melt(id_vars='epoch', var_name='dataset', value_name='NLL')\n",
    "\n",
    "# Plot training and validation loss over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x='epoch', y='NLL', hue='dataset', data=trainhist_melted)\n",
    "plt.ylim(0, trainhist['testing'].quantile(0.999))\n",
    "plt.savefig(f\"plots/{elem}_training_{round(np.corrcoef(holdout['preds'], holdout['obs'])[0, 1]**2, 2)*100}.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Older code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Raster Images\n",
    "def load_rain_data(filepath):\n",
    "    with rasterio.open(filepath) as src:\n",
    "        rain = src.read(1)  # Load the first band\n",
    "        transform = src.transform\n",
    "    return rain, transform\n",
    "\n",
    "# Dynamic path construction using os.path\n",
    "img_variable = \"rainfall_raster_bound.tif\"\n",
    "data_path = os.path.join(\"..\", \"CNN_from_R\", \"images_for_CNN\", img_variable)\n",
    "rain, transform = load_rain_data(data_path)\n",
    "\n",
    "# 2. Load the recharge rate data\n",
    "clor = pd.read_csv(\"../Data/dat07_u.csv\")\n",
    "quant = \"Recharge RC 50% mm/y\" \n",
    "\n",
    "# Drop NaNs for essential columns\n",
    "clor = clor.dropna(subset=[\"lat\", \"lon\", quant])\n",
    "\n",
    "# Convert clor to a GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(clor['lon'], clor['lat'])]\n",
    "clor_gdf = gpd.GeoDataFrame(clor, geometry=geometry, crs=\"EPSG:3577\")  # WGS84 (lat/lon)\n",
    "\n",
    "# Extract the reprojected coordinates\n",
    "clor_gdf['x'] = clor_gdf.geometry.x\n",
    "clor_gdf['y'] = clor_gdf.geometry.y\n",
    "\n",
    "\n",
    "# 3. Extract Sample-Centered Terrain Images\n",
    "def extract_sample_centered_images(clor_gdf, rain, transform, size=32):\n",
    "    rows, cols = rain.shape\n",
    "    half_size = size // 2\n",
    "    terrain_images = []\n",
    "\n",
    "    for _, row in clor_gdf.iterrows():\n",
    "        x, y = row[\"x\"], row[\"y\"]\n",
    "        col, row = ~transform * (x, y)\n",
    "        col, row = int(col), int(row)\n",
    "        \n",
    "        if 0 <= row-half_size < rows and 0 <= col-half_size < cols:\n",
    "            img = rain[row-half_size:row+half_size, col-half_size:col+half_size]\n",
    "            if img.shape == (size, size):\n",
    "                terrain_images.append(img)\n",
    "            else:\n",
    "                terrain_images.append(np.zeros((size, size)))  # Pad with zeros if out of bounds\n",
    "        else:\n",
    "            terrain_images.append(np.zeros((size, size)))  # Completely out of bounds\n",
    "\n",
    "    return np.array(terrain_images)\n",
    "\n",
    "# Use the updated coordinates\n",
    "imgs_ann = extract_sample_centered_images(clor_gdf, rain, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(data_paths) as src:\n",
    "    transform = src.transform\n",
    "    pixel_width = transform[0]   # `a` value: pixel size in x-direction\n",
    "    pixel_height = -transform[4] # `e` value (negated because it's typically negative)\n",
    "    print(f\"Pixel size: {pixel_width} x {pixel_height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(321)\n",
    "fold_size = recharge_data.shape[0] // 10\n",
    "test = np.random.choice(recharge_data.shape[0], fold_size, replace=False)\n",
    "val = np.random.choice(np.setdiff1d(np.arange(recharge_data.shape[0]), test), fold_size, replace=False)\n",
    "train = np.setdiff1d(np.arange(recharge_data.shape[0]), np.concatenate((test, val)))\n",
    "\n",
    "# Data Preparation --------------------------------------------------------\n",
    "\n",
    "x_train = [imgs_ann[train], loc_ann[train]]\n",
    "x_val = [imgs_ann[val], loc_ann[val]]\n",
    "x_test = [imgs_ann[test], loc_ann[test]]\n",
    "\n",
    "x_train[0] = x_train[0].reshape((x_train[0].shape[0], 32, 32, 1))\n",
    "x_val[0] = x_val[0].reshape((x_val[0].shape[0], 32, 32, 1))\n",
    "x_test[0] = x_test[0].reshape((x_test[0].shape[0], 32, 32, 1))\n",
    "\n",
    "\n",
    "y_train = recharge_data['Recharge RC 50% mm/y'].astype(float)[train]\n",
    "y_val = recharge_data['Recharge RC 50% mm/y'].astype(float)[val]\n",
    "y_test = recharge_data['Recharge RC 50% mm/y'].astype(float)[test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
